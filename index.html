<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Neural Radiance Fields (NeRF) Project</title>
<style>
  body {
    font-family: "Inter", "Helvetica", sans-serif;
    margin: 0;
    background: #f8f9fb;
    color: #222;
    line-height: 1.7;
  }
  header {
    background: linear-gradient(135deg, #007bff, #00c4ff);
    color: white;
    text-align: center;
    padding: 60px 20px;
  }
  header h1 { font-size: 2.5em; margin-bottom: 10px; }
  header p { font-size: 1.1em; opacity: 0.9; }

  main {
    max-width: 1000px;
    margin: auto;
    padding: 40px 20px;
  }

  section {
    background: white;
    border-radius: 14px;
    padding: 30px;
    margin-bottom: 40px;
    box-shadow: 0 4px 20px rgba(0,0,0,0.05);
  }
  section h2 {
    color: #007bff;
    border-bottom: 2px solid #eaeaea;
    padding-bottom: 5px;
  }
  h3 {
    color: #444;
    margin-top: 20px;
  }
  ul {
    margin-left: 20px;
  }
  img, video {
    max-width: 100%;
    border-radius: 10px;
    margin: 10px 0;
    box-shadow: 0 2px 8px rgba(0,0,0,0.1);
  }
  .grid {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
    gap: 15px;
  }
  footer {
    text-align: center;
    color: #666;
    padding: 30px;
    font-size: 0.9em;
  }
</style>
</head>

<body>
<header>
  <h1>Neural Radiance Fields (NeRF) Project</h1>
</header>

<main>
<section id="part0">
  <h2>Part 0 ‚Äî Camera Calibration and 3D Scanning</h2>

  <p>
    In this part, I calibrated the camera using ArUco markers, captured multi-view images of the
    target object, estimated camera poses using <code>solvePnP</code>, and constructed the final
    NeRF-ready dataset. The processed images, poses, and intrinsics were saved in <code>.npz</code> format.
  </p>

  <h3>Deliverables</h3>

  <ul>
    <li>Example calibration images and the recovered intrinsic matrix <code>K</code></li>
    <li>Camera pose visualization (Viser frustum screenshots)</li>
    <li>Undistorted vs. original image comparison</li>
  </ul>
  
  <pre><code>Final image shape: (62, 400, 400, 3)
Focal: 386.57093262883313
New intrinsics K:
[[389.6893997    0.         195.09427456]
 [  0.         383.45246556 261.23371531]
 [  0.           0.           1.        ]]
[DONE] saved dataset ‚Üí my_data_square.npz
  </code></pre>
  
  <div class="grid">
    <img src="assets/IMG_6942.heic.jpg" alt=">Example calibration images">
    <img src="assets/undistort_example.png" alt="Undistorted vs. original image comparison">
    <img src="assets/viser_frustums_1.png" alt="Viser frustums top view">
    <img src="assets/viser_frustums_2.png" alt="Viser frustums side view">
  </div>
</section>


<section id="part1" class="section">
  <h2>Part 1 ‚Äî Fit a Neural Field to a 2D Image</h2>

  <p>
    In this section, I trained a <strong>Multi-Layer Perceptron (MLP)</strong> with
    <strong>positional encoding</strong> to learn a continuous 2D neural field representation of an image.
    The network maps 2D coordinates <code>(x, y)</code> to RGB values <code>(r, g, b)</code>, demonstrating
    how neural fields capture high-frequency image details via Fourier feature mappings.
  </p>

  <h3>Model &amp; Training Setup</h3>
  <ul>
    <li><strong>Input:</strong> 2D coordinates normalized to [‚àí1, 1]</li>
    <li><strong>Positional Encoding:</strong> Max frequency <em>L = 10</em>, using sin / cos features</li>
    <li><strong>Network:</strong> 4-layer MLP, hidden width = 256, ReLU activation</li>
    <li><strong>Loss:</strong> Mean Squared Error (MSE)</li>
    <li><strong>Optimizer:</strong> Adam (learning rate = 1e‚àí3)</li>
    <li><strong>Training Steps:</strong> 2000 iterations</li>
  </ul>

  <h3>Training Progress</h3>
  <p>
    The figures below show how the model gradually learns to reconstruct the image over time.
    I visualize intermediate outputs at iterations for both the provided test image and a custom image.
  </p>
  <div class="progress-grid">
    <figure>
      <img src="assets/compare_all.png" alt="Training progression on provided image">
      <figcaption>Provided image ‚Äî reconstruction over iterations.</figcaption>
    </figure>
    <figure>
      <img src="assets/compare_all1.png" alt="Training progression on custom image">
      <figcaption>Custom image ‚Äî reconstruction over iterations.</figcaption>
    </figure>
  </div>

<h3>Hyperparameter Comparison</h3>
<p>
  We compare reconstructions across four settings by varying positional encoding frequency
  (<em>L ‚àà {2, 10}</em>) and network width (<em>{64, 256}</em>). Higher <em>L</em> introduces more
  high-frequency basis functions, enabling sharper edges and finer textures. Wider networks
  provide greater capacity and reduce smoothing artifacts.
</p>

<p>
  Overall trends:
  <ul>
    <li><em>L=2</em>: smoother, blurrier outputs regardless of width.</li>
    <li><em>L=10</em>: noticeably sharper details.</li>
    <li><em>width=64</em>: limited capacity, struggles with fine structure.</li>
    <li><em>width=256</em>: consistently better fidelity.</li>
  </ul>
  The combination <em>L=10, width=256</em> yields the cleanest and most detailed reconstruction.
</p>

  <img src="assets/compare_LW_grid.png" alt="2D Neural Field comparison grid" class="img-large">
  <img src="assets/hyperparameter_comparison.png" alt="2D Neural Field hyperparameter comparison" class="img-large">

  <h3>PSNR Curve</h3>
  <p>
    PSNR steadily improves throughout training. The configuration with <em>L = 10</em> and
    <em>width = 256</em> achieves the highest PSNR of approximately <strong>27.5 dB</strong>,
    indicating high-quality reconstruction.
  </p>
  <img src="assets/psnr_curve.png" alt="PSNR curve for 2D neural field" class="img-medium">

  <h3>Analysis &amp; Summary</h3>
  <p>
    <strong>Observations:</strong> Increasing the positional encoding frequency allows the model
    to capture finer spatial variations, while larger network width increases representational capacity.
    Low-frequency or narrow networks produce smoother, blurrier results. The combination of
    <em>L = 10</em> and <em>width = 256</em> achieves the best balance between fidelity and efficiency.
  </p>
</section>


<section id="part2" class="section">
  <h2>Part 2 ‚Äî Fit a Neural Radiance Field from Multi-view Images</h2>

  <p>
    In this section, I implemented a full <strong>Neural Radiance Field (NeRF)</strong> model trained on
    the <strong>synthetic Lego multi-view dataset</strong>. The goal is to learn a continuous volumetric
    function <em>F(ùíô, d) ‚Üí (œÉ, c)</em> that maps any 3D point <em>ùíô</em> and viewing direction <em>d</em> 
    to density and emitted color. Rendering is done by integrating these quantities along camera rays.
  </p>

  <h3>Part 2.1 ‚Äî Create Rays from Cameras</h3>
  <p>
    I first converted pixels to camera rays by chaining three coordinate transformations:
  </p>
  <ol>
    <li><strong>Camera to World (c2w):</strong> Constructed from extrinsic matrices. The camera origin is <code>c2w[:3, 3]</code>.</li>
    <li><strong>Pixel to Camera:</strong> Used the inverse of intrinsic matrix <code>K‚Åª¬π</code> to transform pixel coordinates (u, v, s) into camera coordinates.</li>
    <li><strong>Pixel to Ray:</strong> Defined each ray by its origin (camera position) and normalized direction (<code>ray_d</code>) obtained from the point at unit depth.</li>
  </ol>
  <p>
    The correctness of the transformation was verified by ensuring <code>x == transform(c2w‚Åª¬π, transform(c2w, x))</code> 
    holds for random points.
  </p>

  <h3>Part 2.2 ‚Äî Sampling</h3>
  <p>
    For each training iteration, I sampled <strong>N = 512</strong> random rays from all training images.
    Along each ray, <strong>n<sub>samples</sub> = 64</strong> points were uniformly sampled between
    <em>near = 2.0</em> and <em>far = 6.0</em>. During training, I added small random perturbations
    to sample locations to improve coverage and reduce overfitting:
  </p>
  <pre><code>t = t + torch.rand_like(t) * (t[1] - t[0])</code></pre>
  <p>
    Each sampled 3D position was computed as <code>x = ray_o + t * ray_d</code>.
  </p>

  <h3>Part 2.3 ‚Äî Dataloader Integration</h3>
  <p>
    I implemented a <code>RaysData</code> class that returns batches of
    <code>(ray_o, ray_d, pixel_color)</code> for training. It converts randomly selected pixel coordinates
    into rays using the methods above. To verify correctness, I visualized the rays, camera frustums,
    and sampled points using <strong>viser</strong>:
  </p>

  <div class="grid">
    <img src="assets/viser_rays.png" alt="Viser visualization of cameras and rays">
    <img src="assets/viser_rays1.png" alt="Viser visualization of cameras and rays">
    <img src="assets/viser_samples.png" alt="Samples along rays visualization">
  </div>

  <p>
    The visualization confirms that sampled rays originate correctly from the cameras and intersect within
    the Lego scene volume.
  </p>

  <h3>Part 2.4 ‚Äî Neural Radiance Field Network</h3>
  <p>
    The NeRF model is composed of an 8-layer MLP with skip connections at layer 4.
    The network takes 3D coordinates and viewing directions as input, both encoded
    with <strong>positional encoding</strong> (<em>L<sub>x</sub>=10</em>, <em>L<sub>d</sub>=4</em>).
    It outputs:
  </p>
  <ul>
    <li><strong>œÉ (density):</strong> ReLU activation ensures non-negative values.</li>
    <li><strong>c (color):</strong> Sigmoid activation constrains color to [0, 1].</li>
  </ul>
  <p>
    The architecture includes a skip connection to help the network preserve spatial information
    across depth layers.
  </p>

  <h3>Part 2.5 ‚Äî Volume Rendering</h3>
  <p>
    I implemented the discrete volume rendering equation to integrate color along each ray:
  </p>
  <pre><code>C = Œ£‚Çú T(t) ¬∑ (1 ‚àí exp(‚àíœÉ‚Çú Œî‚Çú)) ¬∑ c‚Çú,   where  T(t) = exp(‚àíŒ£‚Çñ‚Çç‚Çñ&lt;t‚Çé œÉ‚Çñ Œî‚Çñ)</code></pre>
  <p>
    This was vectorized using <code>torch.cumsum</code> and <code>torch.cumprod</code> for efficiency,
    and verified against the provided test case using random tensors.
  </p>

  <h3>Training Visualization</h3>
  <p>
    The model was trained for 1000 iterations using Adam (lr = 5e‚àí4). I rendered validation
    views during training to visualize progression. The images below show predicted renderings
    at different stages of optimization:
  </p>

  <div class="grid">
    <img src="assets/train_step_100.png" alt="Iter prediction">
    <img src="assets/train_step_500.png" alt="Frames">
  </div>

  <p>
    <strong>PSNR Curve:</strong> The validation PSNR steadily improves, reaching ‚âà26 dB after 3000 steps.
  </p>
  <img src="assets/psnr_curve1.png" alt="PSNR curve for Lego NeRF" class="img-medium">

  <h3>Spherical Rendering Video</h3>
  <p>
    Using the provided <code>c2ws_test</code> camera poses, I rendered novel views of the Lego
    scene along a circular trajectory. The final video demonstrates smooth 3D reconstruction
    and consistent lighting across viewpoints.
  </p>
  <img src="assets/spherical_render_26dB.gif" alt="render view">
</section>


<section id="part26" class="section">
  <h2>Part 2.6 ‚Äî Training NeRF on My Own Captured Data</h2>

  <p>
    For this section, I captured my own real-world object using a handheld camera and generated
    training data through COLMAP reconstruction. After converting the poses into NeRF-compatible
    format, I trained a full NeRF model using the same volume-rendering pipeline from Part 2,
    but with modified hyperparameters to better fit real-world data.
  </p>

  <h3>Training Setup</h3>
  <p>
    The model used a deeper TinyNeRF architecture:
  </p>
  <ul>
    <li><strong>Network:</strong> TinyNeRF with width 256, positional encoding L<sub>xyz</sub>=10, L<sub>dir</sub>=4</li>
    <li><strong>Ray batch size:</strong> 1024 rays per iteration</li>
    <li><strong>Sampling:</strong> 64 points per ray, near=0.02, far=0.5</li>
    <li><strong>Training iterations:</strong> 20,000</li>
    <li><strong>Optimizer:</strong> Adam with learning rate decay (Œ≥=0.995)</li>
  </ul>

  <h3>Training Loss & Validation PSNR</h3>
  <p>
    I recorded the full training loss curve and periodic PSNR evaluation on held-out poses.It takes about 6 hours and reached 26DB PSNR, but it is still a bit rough.
  </p>
  <img src="assets/loss.png" alt="Training loss curve and Validation PSNR curve" class="img-medium">

  <h3>Intermediate Rendered Views</h3>
  <p>
    Throughout training, I periodically rendered the validation image to visualize convergence.
    Below are selected snapshots at different training iterations:
  </p>
  <div class="grid">
    <img src="assets/intermediate.png" alt="iter render">
     <img src="assets/frames.png" alt="frames">
  </div>

  <h3>Ordered Training-View Renderings (GIF)</h3>
  <p>
    Using the provided <code>c2ws_test</code> camera poses, I rendered novel views of my
    scene along a circular trajectory. The final video demonstrates smooth 3D reconstruction
    and consistent lighting across viewpoints.
  </p>
  <img src="assets/ordered_training_views1.GIF" alt="Ordered training views GIF" class="img-large">

</section>


</main>
</body>
</html>
