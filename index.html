<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Neural Radiance Fields (NeRF) Project</title>
<style>
  body {
    font-family: "Inter", "Helvetica", sans-serif;
    margin: 0;
    background: #f8f9fb;
    color: #222;
    line-height: 1.7;
  }
  header {
    background: linear-gradient(135deg, #007bff, #00c4ff);
    color: white;
    text-align: center;
    padding: 60px 20px;
  }
  header h1 { font-size: 2.5em; margin-bottom: 10px; }
  header p { font-size: 1.1em; opacity: 0.9; }

  main {
    max-width: 1000px;
    margin: auto;
    padding: 40px 20px;
  }

  section {
    background: white;
    border-radius: 14px;
    padding: 30px;
    margin-bottom: 40px;
    box-shadow: 0 4px 20px rgba(0,0,0,0.05);
  }
  section h2 {
    color: #007bff;
    border-bottom: 2px solid #eaeaea;
    padding-bottom: 5px;
  }
  h3 {
    color: #444;
    margin-top: 20px;
  }
  ul {
    margin-left: 20px;
  }
  img, video {
    max-width: 100%;
    border-radius: 10px;
    margin: 10px 0;
    box-shadow: 0 2px 8px rgba(0,0,0,0.1);
  }
  .grid {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
    gap: 15px;
  }
  footer {
    text-align: center;
    color: #666;
    padding: 30px;
    font-size: 0.9em;
  }
</style>
</head>

<body>
<header>
  <h1>Neural Radiance Fields (NeRF) Project</h1>
  <p>CS180 / CS280 - Computer Vision & Computational Imaging</p>
  <p><strong>Author:</strong> Your Name | <strong>Semester:</strong> Fall 2025</p>
</header>

<main>
  <section id="overview">
    <h2>Overview</h2>
    <p>
      This project implements a full Neural Radiance Field (NeRF) pipeline from scratch,
      following the assignments of <strong>CS180/CS280</strong>.
      It covers camera calibration, 2D neural field fitting, and 3D neural rendering with
      both synthetic (Lego) and real-world data.
    </p>
  </section>

  <section id="part0">
    <h2>Part 0 ‚Äî Camera Calibration and 3D Scanning</h2>
    <p>
      In this stage, we used <strong>COLMAP</strong> and <strong>Viser</strong> to estimate camera poses and reconstruct
      a sparse point cloud from multi-view images. The results were exported as a custom dataset
      in <code>.npz</code> format for NeRF training.
    </p>
    <h3>Deliverables</h3>
    <ul>
      <li>Two screenshots of Viser visualizing camera frustums.</li>
      <li>Generated <code>.npz</code> dataset (images, c2ws, focal).</li>
    </ul>
    <div class="grid">
      <img src="assets/viser_frustums_1.png" alt="Viser frustums 1">
      <img src="assets/viser_frustums_2.png" alt="Viser frustums 2">
    </div>
  </section>

<section id="part1" class="section">
  <h2>Part 1 ‚Äî Fit a Neural Field to a 2D Image</h2>

  <p>
    In this section, we trained a <strong>Multi-Layer Perceptron (MLP)</strong> with
    <strong>positional encoding</strong> to learn a continuous 2D neural field representation of an image.
    The network maps 2D coordinates <code>(x, y)</code> to RGB values <code>(r, g, b)</code>, demonstrating
    how neural fields capture high-frequency image details via Fourier feature mappings.
  </p>

  <h3>Model &amp; Training Setup</h3>
  <ul>
    <li><strong>Input:</strong> 2D coordinates normalized to [‚àí1, 1]</li>
    <li><strong>Positional Encoding:</strong> Max frequency <em>L = 10</em>, using sin / cos features</li>
    <li><strong>Network:</strong> 4-layer MLP, hidden width = 256, ReLU activation</li>
    <li><strong>Loss:</strong> Mean Squared Error (MSE)</li>
    <li><strong>Optimizer:</strong> Adam (learning rate = 1e‚àí3)</li>
    <li><strong>Training Steps:</strong> 2000 iterations</li>
  </ul>

  <h3>Training Progress</h3>
  <p>
    The figures below show how the model gradually learns to reconstruct the image over time.
    We visualize intermediate outputs at iterations <strong>1, 200, 1000,</strong> and <strong>2000</strong>
    for both the provided test image and a custom image.
  </p>
  <div class="progress-grid">
    <figure>
      <img src="assets/compare_all.png" alt="Training progression on provided image">
      <figcaption>Provided image ‚Äî reconstruction over iterations.</figcaption>
    </figure>
    <figure>
      <img src="assets/compare_all1.png" alt="Training progression on custom image">
      <figcaption>Custom image ‚Äî reconstruction over iterations.</figcaption>
    </figure>
  </div>

  <h3>Hyperparameter Comparison</h3>
  <p>
    The following grids compare reconstructions across different combinations of positional encoding
    frequency (<em>L ‚àà {2, 10}</em>) and network width (<em>{64, 256}</em>).
    Higher frequencies and wider networks yield sharper, more detailed reconstructions.
  </p>
  <img src="assets/compare_LW_grid.png" alt="2D Neural Field comparison grid" class="img-large">
  <img src="assets/hyperparameter_comparison.png" alt="2D Neural Field hyperparameter comparison" class="img-large">

  <h3>PSNR Curve</h3>
  <p>
    PSNR steadily improves throughout training. The configuration with <em>L = 10</em> and
    <em>width = 256</em> achieves the highest PSNR of approximately <strong>32 dB</strong>,
    indicating high-quality reconstruction.
  </p>
  <img src="assets/psnr_curve.png" alt="PSNR curve for 2D neural field" class="img-medium">

  <h3>Analysis &amp; Summary</h3>
  <p>
    <strong>Observations:</strong> Increasing the positional encoding frequency allows the model
    to capture finer spatial variations, while larger network width increases representational capacity.
    Low-frequency or narrow networks produce smoother, blurrier results. The combination of
    <em>L = 10</em> and <em>width = 256</em> achieves the best balance between fidelity and efficiency.
  </p>
  <ul>
    <li><strong>Model description:</strong> Network depth, width, and positional encoding setup.</li>
    <li><strong>Training progression:</strong> Intermediate reconstructions on both provided and custom images.</li>
    <li><strong>Reconstruction results:</strong> Final 2√ó2 grid comparison of different L and width combinations.</li>
    <li><strong>PSNR curve:</strong> Quantitative convergence trend during training.</li>
    <li><strong>Analysis:</strong> Discussion of how encoding frequency and network size affect image quality.</li>
  </ul>
</section>


<section id="part2" class="section">
  <h2>Part 2 ‚Äî Fit a Neural Radiance Field from Multi-view Images</h2>

  <p>
    In this section, we implemented a full <strong>Neural Radiance Field (NeRF)</strong> model trained on
    the <strong>synthetic Lego multi-view dataset</strong>. The goal is to learn a continuous volumetric
    function <em>F(ùíô, d) ‚Üí (œÉ, c)</em> that maps any 3D point <em>ùíô</em> and viewing direction <em>d</em> 
    to density and emitted color. Rendering is done by integrating these quantities along camera rays.
  </p>

  <h3>Part 2.1 ‚Äî Create Rays from Cameras</h3>
  <p>
    We first converted pixels to camera rays by chaining three coordinate transformations:
  </p>
  <ol>
    <li><strong>Camera to World (c2w):</strong> Constructed from extrinsic matrices. The camera origin is <code>c2w[:3, 3]</code>.</li>
    <li><strong>Pixel to Camera:</strong> Used the inverse of intrinsic matrix <code>K‚Åª¬π</code> to transform pixel coordinates (u, v, s) into camera coordinates.</li>
    <li><strong>Pixel to Ray:</strong> Defined each ray by its origin (camera position) and normalized direction (<code>ray_d</code>) obtained from the point at unit depth.</li>
  </ol>
  <p>
    The correctness of the transformation was verified by ensuring <code>x == transform(c2w‚Åª¬π, transform(c2w, x))</code> 
    holds for random points.
  </p>

  <h3>Part 2.2 ‚Äî Sampling</h3>
  <p>
    For each training iteration, we sampled <strong>N = 512</strong> random rays from all training images.
    Along each ray, <strong>n<sub>samples</sub> = 64</strong> points were uniformly sampled between
    <em>near = 2.0</em> and <em>far = 6.0</em>. During training, we added small random perturbations
    to sample locations to improve coverage and reduce overfitting:
  </p>
  <pre><code>t = t + torch.rand_like(t) * (t[1] - t[0])</code></pre>
  <p>
    Each sampled 3D position was computed as <code>x = ray_o + t * ray_d</code>.
  </p>

  <h3>Part 2.3 ‚Äî Dataloader Integration</h3>
  <p>
    We implemented a <code>RaysData</code> class that returns batches of
    <code>(ray_o, ray_d, pixel_color)</code> for training. It converts randomly selected pixel coordinates
    into rays using the methods above. To verify correctness, we visualized the rays, camera frustums,
    and sampled points using <strong>viser</strong>:
  </p>

  <div class="grid">
    <img src="assets/viser_rays.png" alt="Viser visualization of cameras and rays">
    <img src="assets/viser_samples.png" alt="Samples along rays visualization">
  </div>

  <p>
    The visualization confirms that sampled rays originate correctly from the cameras and intersect within
    the Lego scene volume.
  </p>

  <h3>Part 2.4 ‚Äî Neural Radiance Field Network</h3>
  <p>
    The NeRF model is composed of an 8-layer MLP with skip connections at layer 4.
    The network takes 3D coordinates and viewing directions as input, both encoded
    with <strong>positional encoding</strong> (<em>L<sub>x</sub>=10</em>, <em>L<sub>d</sub>=4</em>).
    It outputs:
  </p>
  <ul>
    <li><strong>œÉ (density):</strong> ReLU activation ensures non-negative values.</li>
    <li><strong>c (color):</strong> Sigmoid activation constrains color to [0, 1].</li>
  </ul>
  <p>
    The architecture includes a skip connection to help the network preserve spatial information
    across depth layers.
  </p>

  <h3>Part 2.5 ‚Äî Volume Rendering</h3>
  <p>
    We implemented the discrete volume rendering equation to integrate color along each ray:
  </p>
  <pre><code>C = Œ£‚Çú T(t) ¬∑ (1 ‚àí exp(‚àíœÉ‚Çú Œî‚Çú)) ¬∑ c‚Çú,   where  T(t) = exp(‚àíŒ£‚Çñ‚Çç‚Çñ&lt;t‚Çé œÉ‚Çñ Œî‚Çñ)</code></pre>
  <p>
    This was vectorized using <code>torch.cumsum</code> and <code>torch.cumprod</code> for efficiency,
    and verified against the provided test case using random tensors.
  </p>

  <h3>Training Visualization</h3>
  <p>
    The model was trained for 1000 iterations using Adam (lr = 5e‚àí4). We rendered validation
    views during training to visualize progression. The images below show predicted renderings
    at different stages of optimization:
  </p>

  <div class="grid">
    <img src="assets/train_step_100.png" alt="Step 100 prediction">
    <img src="assets/train_step_500.png" alt="Step 500 prediction">
    <img src="assets/train_step_1000.png" alt="Step 1000 prediction">
  </div>

  <p>
    <strong>PSNR Curve:</strong> The validation PSNR steadily improves, reaching ‚âà30 dB after 1000 steps.
  </p>
  <img src="assets/psnr_nerf.png" alt="PSNR curve for Lego NeRF" class="img-medium">

  <h3>Spherical Rendering Video</h3>
  <p>
    Using the provided <code>c2ws_test</code> camera poses, we rendered novel views of the Lego
    scene along a circular trajectory. The final video demonstrates smooth 3D reconstruction
    and consistent lighting across viewpoints.
  </p>
  <video controls class="video-large">
    <source src="assets/lego_render.mp4" type="video/mp4">
    Your browser does not support video playback.
  </video>

  <h3>Summary & Analysis</h3>
  <p>
    This part demonstrates how multi-view supervision enables learning of a volumetric scene
    representation through differentiable rendering. The results show accurate geometry recovery,
    high-frequency texture reconstruction, and smooth view interpolation, validating the full
    NeRF pipeline implementation.
  </p>
</section>


  <section id="part26">
    <h2>Part 2.6 ‚Äî Training NeRF with My Own Data</h2>
    <p>
      Captured a real-world object using multiple camera poses and trained a NeRF model
      from scratch. Rendered novel views demonstrate generalization and geometry recovery.
    </p>
    <h3>Deliverables</h3>
    <ul>
      <li>GIF showing novel view renderings of the object.</li>
      <li>Loss curve over iterations.</li>
      <li>Intermediate render results.</li>
      <li>Notes on modified hyperparameters or architecture.</li>
    </ul>
    <div class="grid">
      <img src="assets/my_object.gif" alt="My object novel view">
      <img src="assets/loss_curve.png" alt="Loss curve">
      <img src="assets/intermediate_render_1.png" alt="Intermediate render">
    </div>
  </section>

  <section id="conclusion">
    <h2>Conclusion</h2>
    <p>
      Through this project, I implemented the complete NeRF pipeline and visualized how
      neural networks can represent 3D scenes from 2D observations.
      The experiment demonstrates the power of coordinate-based MLPs and differentiable
      volume rendering for novel view synthesis.
    </p>
  </section>
</main>

<footer>
  ¬© 2025 Your Name ‚Äî UC Berkeley GLOBE / CS180 NeRF Project
</footer>
</body>
</html>
