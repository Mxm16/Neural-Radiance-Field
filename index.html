<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Neural Radiance Fields (NeRF) Project</title>
<style>
  body {
    font-family: "Inter", "Helvetica", sans-serif;
    margin: 0;
    background: #f8f9fb;
    color: #222;
    line-height: 1.7;
  }
  header {
    background: linear-gradient(135deg, #007bff, #00c4ff);
    color: white;
    text-align: center;
    padding: 60px 20px;
  }
  header h1 { font-size: 2.5em; margin-bottom: 10px; }
  header p { font-size: 1.1em; opacity: 0.9; }

  main {
    max-width: 1000px;
    margin: auto;
    padding: 40px 20px;
  }

  section {
    background: white;
    border-radius: 14px;
    padding: 30px;
    margin-bottom: 40px;
    box-shadow: 0 4px 20px rgba(0,0,0,0.05);
  }
  section h2 {
    color: #007bff;
    border-bottom: 2px solid #eaeaea;
    padding-bottom: 5px;
  }
  h3 {
    color: #444;
    margin-top: 20px;
  }
  ul {
    margin-left: 20px;
  }
  img, video {
    max-width: 100%;
    border-radius: 10px;
    margin: 10px 0;
    box-shadow: 0 2px 8px rgba(0,0,0,0.1);
  }
  .grid {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
    gap: 15px;
  }
  footer {
    text-align: center;
    color: #666;
    padding: 30px;
    font-size: 0.9em;
  }
</style>
</head>

<body>
<header>
  <h1>Neural Radiance Fields (NeRF) Project</h1>
  <p>CS180 / CS280 - Computer Vision & Computational Imaging</p>
  <p><strong>Author:</strong> Your Name | <strong>Semester:</strong> Fall 2025</p>
</header>

<main>
  <section id="overview">
    <h2>Overview</h2>
    <p>
      This project implements a full Neural Radiance Field (NeRF) pipeline from scratch,
      following the assignments of <strong>CS180/CS280</strong>.
      It covers camera calibration, 2D neural field fitting, and 3D neural rendering with
      both synthetic (Lego) and real-world data.
    </p>
  </section>

  <section id="part0">
    <h2>Part 0 — Camera Calibration and 3D Scanning</h2>
    <p>
      In this stage, we used <strong>COLMAP</strong> and <strong>Viser</strong> to estimate camera poses and reconstruct
      a sparse point cloud from multi-view images. The results were exported as a custom dataset
      in <code>.npz</code> format for NeRF training.
    </p>
    <h3>Deliverables</h3>
    <ul>
      <li>Two screenshots of Viser visualizing camera frustums.</li>
      <li>Generated <code>.npz</code> dataset (images, c2ws, focal).</li>
    </ul>
    <div class="grid">
      <img src="assets/viser_frustums_1.png" alt="Viser frustums 1">
      <img src="assets/viser_frustums_2.png" alt="Viser frustums 2">
    </div>
  </section>

<section id="part1" class="section">
  <h2>Part 1 — Fit a Neural Field to a 2D Image</h2>

  <p>
    In this section, we trained a <strong>Multi-Layer Perceptron (MLP)</strong> with
    <strong>positional encoding</strong> to learn a continuous 2D neural field representation of an image.
    The network maps 2D coordinates <code>(x, y)</code> to RGB values <code>(r, g, b)</code>, demonstrating
    how neural fields capture high-frequency image details via Fourier feature mappings.
  </p>

  <h3>Model &amp; Training Setup</h3>
  <ul>
    <li><strong>Input:</strong> 2D coordinates normalized to [−1, 1]</li>
    <li><strong>Positional Encoding:</strong> Max frequency <em>L = 10</em>, using sin / cos features</li>
    <li><strong>Network:</strong> 4-layer MLP, hidden width = 256, ReLU activation</li>
    <li><strong>Loss:</strong> Mean Squared Error (MSE)</li>
    <li><strong>Optimizer:</strong> Adam (learning rate = 1e−3)</li>
    <li><strong>Training Steps:</strong> 2000 iterations</li>
  </ul>

  <h3>Training Progress</h3>
  <p>
    The figures below show how the model gradually learns to reconstruct the image over time.
    We visualize intermediate outputs at iterations <strong>1, 200, 1000,</strong> and <strong>2000</strong>
    for both the provided test image and a custom image.
  </p>
  <div class="progress-grid">
    <figure>
      <img src="assets/compare_all.png" alt="Training progression on provided image">
      <figcaption>Provided image — reconstruction over iterations.</figcaption>
    </figure>
    <figure>
      <img src="assets/compare_all1.png" alt="Training progression on custom image">
      <figcaption>Custom image — reconstruction over iterations.</figcaption>
    </figure>
  </div>

  <h3>Hyperparameter Comparison</h3>
  <p>
    The following grids compare reconstructions across different combinations of positional encoding
    frequency (<em>L ∈ {2, 10}</em>) and network width (<em>{64, 256}</em>).
    Higher frequencies and wider networks yield sharper, more detailed reconstructions.
  </p>
  <img src="assets/compare_LW_grid.png" alt="2D Neural Field comparison grid" class="img-large">
  <img src="assets/hyperparameter_comparison.png" alt="2D Neural Field hyperparameter comparison" class="img-large">

  <h3>PSNR Curve</h3>
  <p>
    PSNR steadily improves throughout training. The configuration with <em>L = 10</em> and
    <em>width = 256</em> achieves the highest PSNR of approximately <strong>32 dB</strong>,
    indicating high-quality reconstruction.
  </p>
  <img src="assets/psnr_curve.png" alt="PSNR curve for 2D neural field" class="img-medium">

  <h3>Analysis &amp; Summary</h3>
  <p>
    <strong>Observations:</strong> Increasing the positional encoding frequency allows the model
    to capture finer spatial variations, while larger network width increases representational capacity.
    Low-frequency or narrow networks produce smoother, blurrier results. The combination of
    <em>L = 10</em> and <em>width = 256</em> achieves the best balance between fidelity and efficiency.
  </p>
  <ul>
    <li><strong>Model description:</strong> Network depth, width, and positional encoding setup.</li>
    <li><strong>Training progression:</strong> Intermediate reconstructions on both provided and custom images.</li>
    <li><strong>Reconstruction results:</strong> Final 2×2 grid comparison of different L and width combinations.</li>
    <li><strong>PSNR curve:</strong> Quantitative convergence trend during training.</li>
    <li><strong>Analysis:</strong> Discussion of how encoding frequency and network size affect image quality.</li>
  </ul>
</section>


  <section id="part2">
    <h2>Part 2 — Fit a Neural Radiance Field (Lego Scene)</h2>
    <p>
      Implemented a full NeRF model with volumetric rendering. Trained on the
      <strong>Lego synthetic dataset</strong> to learn a 3D radiance field.
      Each rendered image is generated by integrating densities and colors along camera rays.
    </p>
    <h3>Deliverables</h3>
    <ul>
      <li>Implementation details of Parts 2.1–2.5 (rays, sampling, NeRF model, volume rendering).</li>
      <li>Viser visualization of sampled points and rays.</li>
      <li>Predicted render vs ground truth images.</li>
      <li>PSNR curve for training.</li>
      <li>Spherical rendering video of the Lego scene.</li>
    </ul>
    <div class="grid">
      <img src="assets/viser_rays.png" alt="Rays visualization">
      <img src="assets/psnr_nerf.png" alt="NeRF PSNR curve">
    </div>
    <h3>Spherical Render Video</h3>
    <video controls>
      <source src="assets/lego_render.mp4" type="video/mp4">
      Your browser does not support video playback.
    </video>
  </section>

  <section id="part26">
    <h2>Part 2.6 — Training NeRF with My Own Data</h2>
    <p>
      Captured a real-world object using multiple camera poses and trained a NeRF model
      from scratch. Rendered novel views demonstrate generalization and geometry recovery.
    </p>
    <h3>Deliverables</h3>
    <ul>
      <li>GIF showing novel view renderings of the object.</li>
      <li>Loss curve over iterations.</li>
      <li>Intermediate render results.</li>
      <li>Notes on modified hyperparameters or architecture.</li>
    </ul>
    <div class="grid">
      <img src="assets/my_object.gif" alt="My object novel view">
      <img src="assets/loss_curve.png" alt="Loss curve">
      <img src="assets/intermediate_render_1.png" alt="Intermediate render">
    </div>
  </section>

  <section id="conclusion">
    <h2>Conclusion</h2>
    <p>
      Through this project, I implemented the complete NeRF pipeline and visualized how
      neural networks can represent 3D scenes from 2D observations.
      The experiment demonstrates the power of coordinate-based MLPs and differentiable
      volume rendering for novel view synthesis.
    </p>
  </section>
</main>

<footer>
  © 2025 Your Name — UC Berkeley GLOBE / CS180 NeRF Project
</footer>
</body>
</html>
