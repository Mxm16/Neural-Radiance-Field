<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Neural Radiance Fields (NeRF) Project</title>
<style>
  body {
    font-family: "Inter", "Helvetica", sans-serif;
    margin: 0;
    background: #f8f9fb;
    color: #222;
    line-height: 1.7;
  }
  header {
    background: linear-gradient(135deg, #007bff, #00c4ff);
    color: white;
    text-align: center;
    padding: 60px 20px;
  }
  header h1 { font-size: 2.5em; margin-bottom: 10px; }
  header p { font-size: 1.1em; opacity: 0.9; }

  main {
    max-width: 1000px;
    margin: auto;
    padding: 40px 20px;
  }

  section {
    background: white;
    border-radius: 14px;
    padding: 30px;
    margin-bottom: 40px;
    box-shadow: 0 4px 20px rgba(0,0,0,0.05);
  }
  section h2 {
    color: #007bff;
    border-bottom: 2px solid #eaeaea;
    padding-bottom: 5px;
  }
  h3 {
    color: #444;
    margin-top: 20px;
  }
  ul {
    margin-left: 20px;
  }
  img, video {
    max-width: 100%;
    border-radius: 10px;
    margin: 10px 0;
    box-shadow: 0 2px 8px rgba(0,0,0,0.1);
  }
  .grid {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
    gap: 15px;
  }
  footer {
    text-align: center;
    color: #666;
    padding: 30px;
    font-size: 0.9em;
  }
</style>
</head>

<body>
<header>
  <h1>Neural Radiance Fields (NeRF) Project</h1>
  <p>CS180 / CS280 - Computer Vision & Computational Imaging</p>
  <p><strong>Author:</strong> Your Name | <strong>Semester:</strong> Fall 2025</p>
</header>

<main>
  <section id="overview">
    <h2>Overview</h2>
    <p>
      This project implements a full Neural Radiance Field (NeRF) pipeline from scratch,
      following the assignments of <strong>CS180/CS280</strong>.
      It covers camera calibration, 2D neural field fitting, and 3D neural rendering with
      both synthetic (Lego) and real-world data.
    </p>
  </section>

  <section id="part0">
    <h2>Part 0 — Camera Calibration and 3D Scanning</h2>
    <p>
      In this stage, we used <strong>COLMAP</strong> and <strong>Viser</strong> to estimate camera poses and reconstruct
      a sparse point cloud from multi-view images. The results were exported as a custom dataset
      in <code>.npz</code> format for NeRF training.
    </p>
    <h3>Deliverables</h3>
    <ul>
      <li>Two screenshots of Viser visualizing camera frustums.</li>
      <li>Generated <code>.npz</code> dataset (images, c2ws, focal).</li>
    </ul>
    <div class="grid">
      <img src="assets/viser_frustums_1.png" alt="Viser frustums 1">
      <img src="assets/viser_frustums_2.png" alt="Viser frustums 2">
    </div>
  </section>

  <section id="part1" class="section">
  <h2>Part 1 — Fit a Neural Field to a 2D Image</h2>

  <p>
    In this section, we trained a <strong>Multi-Layer Perceptron (MLP)</strong> with
    <strong>positional encoding</strong> to learn a continuous 2D neural field representation of an image.
    The network maps 2D coordinates <code>(x, y)</code> to RGB values <code>(r, g, b)</code>, demonstrating
    how neural fields capture high-frequency image details via Fourier feature mappings.
  </p>

  <h3>Model &amp; Training Setup</h3>
  <ul>
    <li><strong>Input:</strong> 2D coordinates normalized to [−1, 1]</li>
    <li><strong>Positional Encoding:</strong> Max frequency <em>L = 10</em>, using sin / cos features</li>
    <li><strong>Network:</strong> 4-layer MLP, hidden width = 256, ReLU activation</li>
    <li><strong>Loss:</strong> Mean Squared Error (MSE)</li>
    <li><strong>Optimizer:</strong> Adam (learning rate = 1e−3)</li>
    <li><strong>Training Steps:</strong> 2000 iterations</li>
  </ul>

  <h3>Results</h3>
  <p>
    <strong>Model Architecture:</strong> A 4-layer MLP with hidden width 256, taking positional encoded
    coordinates as input and outputting RGB values. Positional encoding with L=10 allows the model
    to represent high-frequency image structures.
  </p>
  <p>
    <strong>Reconstructed Images:</strong> We saved intermediate and final reconstructions. The
    following 2×2 grid shows the effect of different positional encoding frequencies and network widths:
  </p>
  <img src="assets/result_grid_2x2.png" alt="2D Neural Field grid comparison" class="img-large">

  <p>
    <strong>PSNR Curve:</strong> PSNR improves steadily across training iterations. For the best configuration
    (L=10, width=256), PSNR reaches around 32 dB, indicating high reconstruction quality.
  </p>
  <img src="assets/psnr_curve.png" alt="PSNR curve for 2D neural field" class="img-medium">

  <p>
    <strong>Analysis:</strong> Higher positional encoding frequencies capture finer spatial details,
    while wider networks provide more representational capacity. Low-frequency or narrow networks produce
    blurrier reconstructions. This demonstrates the trade-off between model capacity and positional encoding.
  </p>

  <h3>Summary of Deliverables Shown on This Page</h3>
  <ul>
    <li><strong>Model description:</strong> Input size, network depth & width, activation functions, learning rate.</li>
    <li><strong>Training progression:</strong> PSNR curve visualizing model convergence over iterations.</li>
    <li><strong>Reconstruction results:</strong> Final reconstructed images for all 2×2 combinations of L and width.</li>
    <li><strong>Comparative analysis:</strong> Observations about how frequency and width affect reconstruction quality.</li>
  </ul>
</section>


  <section id="part2">
    <h2>Part 2 — Fit a Neural Radiance Field (Lego Scene)</h2>
    <p>
      Implemented a full NeRF model with volumetric rendering. Trained on the
      <strong>Lego synthetic dataset</strong> to learn a 3D radiance field.
      Each rendered image is generated by integrating densities and colors along camera rays.
    </p>
    <h3>Deliverables</h3>
    <ul>
      <li>Implementation details of Parts 2.1–2.5 (rays, sampling, NeRF model, volume rendering).</li>
      <li>Viser visualization of sampled points and rays.</li>
      <li>Predicted render vs ground truth images.</li>
      <li>PSNR curve for training.</li>
      <li>Spherical rendering video of the Lego scene.</li>
    </ul>
    <div class="grid">
      <img src="assets/viser_rays.png" alt="Rays visualization">
      <img src="assets/psnr_nerf.png" alt="NeRF PSNR curve">
    </div>
    <h3>Spherical Render Video</h3>
    <video controls>
      <source src="assets/lego_render.mp4" type="video/mp4">
      Your browser does not support video playback.
    </video>
  </section>

  <section id="part26">
    <h2>Part 2.6 — Training NeRF with My Own Data</h2>
    <p>
      Captured a real-world object using multiple camera poses and trained a NeRF model
      from scratch. Rendered novel views demonstrate generalization and geometry recovery.
    </p>
    <h3>Deliverables</h3>
    <ul>
      <li>GIF showing novel view renderings of the object.</li>
      <li>Loss curve over iterations.</li>
      <li>Intermediate render results.</li>
      <li>Notes on modified hyperparameters or architecture.</li>
    </ul>
    <div class="grid">
      <img src="assets/my_object.gif" alt="My object novel view">
      <img src="assets/loss_curve.png" alt="Loss curve">
      <img src="assets/intermediate_render_1.png" alt="Intermediate render">
    </div>
  </section>

  <section id="conclusion">
    <h2>Conclusion</h2>
    <p>
      Through this project, I implemented the complete NeRF pipeline and visualized how
      neural networks can represent 3D scenes from 2D observations.
      The experiment demonstrates the power of coordinate-based MLPs and differentiable
      volume rendering for novel view synthesis.
    </p>
  </section>
</main>

<footer>
  © 2025 Your Name — UC Berkeley GLOBE / CS180 NeRF Project
</footer>
</body>
</html>
